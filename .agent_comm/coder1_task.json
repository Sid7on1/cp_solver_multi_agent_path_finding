{
  "agent_id": "coder1",
  "task_id": "task_1",
  "files": [
    {
      "filename": "main.py",
      "purpose": "Entry point for the CP-Solver application",
      "priority": "high",
      "dependencies": [
        "torch",
        "numpy",
        "pandas"
      ],
      "key_functions": [
        "load_data",
        "train_model",
        "run_cp_solver"
      ],
      "estimated_lines": 500,
      "complexity": "high"
    }
  ],
  "project_info": {
    "project_name": "CP_Solver_Multi_Agent_Path_Finding",
    "project_type": "reinforcement_learning",
    "description": "Implementation of CP-Solver for Multi-Agent Path Finding among Dynamic Uncontrollable Agents with Statistical Safety Guarantees",
    "key_algorithms": [
      "Conformal_Prediction",
      "Enhanced_Conflict_Based_Search",
      "Long_Short_Term_Memory_Networks"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas",
      "scikit_learn"
    ]
  },
  "paper_content": "PDF: cs.MA_2507.22282v1_Multi-Agent-Path-Finding-Among-Dynamic-Uncontrolla.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nMulti-Agent Path Finding Among Dynamic Uncontrollable Agents\nwith Statistical Safety Guarantees\nKegan J. Strawn1, Thomy Phan3, Eric Wang1, Nora Ayanian2, Sven Koenig3, Lars Lindemann1\n1University of Southern California\n2Brown University\n3University of California, Irvine\n{k.j.strawn, llindema }@usc.edu, nora ayanian@brown.edu, {thomyp, sven.koenig }@uci.edu\nAbstract\nExisting multi-agent path finding (MAPF) solvers do not ac-\ncount for uncertain behavior of uncontrollable agents. We\npresent a novel variant of Enhanced Conflict-Based Search\n(ECBS), for both one-shot and lifelong MAPF in dynamic en-\nvironments with uncontrollable agents. Our method consists\nof (1) training a learned predictor for the movement of un-\ncontrollable agents, (2) quantifying the prediction error using\nconformal prediction (CP), a tool for statistical uncertainty\nquantification, and (3) integrating these uncertainty intervals\ninto our modified ECBS solver. Our method can account for\nuncertain agent behavior, comes with statistical guarantees\non collision-free paths for one-shot missions, and scales to\nlifelong missions with a receding horizon sequence of one-\nshot instances. We run our algorithm, CP-Solver, across ware-\nhouse and game maps, with competitive throughput and re-\nduced collisions.\nIntroduction\nMulti-Agent Path Finding (MAPF), where multiple agents\nmust navigate to goal locations without collisions, has broad\napplications in robotics, video games, and logistics (LaValle\n2006; Yu and LaValle 2016; Stern et al. 2019). While MAPF\nalgorithms are effective in one-shot (start-to-goal) static\nenvironments, they do not handle settings where agents\nshare the environment with uncontrollable, non-cooperative\nagents, such as humans or player-controlled agents in a\ngame. This paper focuses on MAPF for controllable agents\nthat interact with these uncontrollable agents whose plans\nand behaviors are unknown. We provide an approach appli-\ncable to the one-shot and lifelong variants of MAPF where,\nin lifelong MAPF, the agents attend to a potentially endless\nsequence of goals (Ma et al. 2017). Specifically:\n\u2022 We formulate the problem of MAPF among Dynamic\nUncontrollable Agents (DUA).\n\u2022 We propose CP-Solver which combines a (learned) pre-\ndictive model of uncontrollable agents and conformal\nprediction (CP) with Enhanced Conflict-Based Search\n(ECBS). CP-solver equips predictions with uncertainty\nintervals and prioritizes predictions during ECBS conflict\nresolution, see Figure 1.\n\u2022 We present two variants of CP-Solver: open-loop for one-\nshot DUA and closed-loop for lifelong DUA.\nFigure 1: CP-Solver plans paths across a mission interval\n[t, T]for controlled agents a\u2208A:={4,5,6,7,8,9}from\nstarting locations \u03c8s,a(blue circles) to goal locations \u03c8g,a\n(yellow circles) such that they avoid uncontrollable agents\nb\u2208U:={0,1,2,3}(solid red circles) with probabil-\nity1\u2212\u03b4. This is achieved via predictions \u00afB(i)\nt+1:t+H(red\nlines) and uncertainty intervals Ct+1:t+H(red shaded cir-\ncles) computed from observations \u02dcB(i)\n0:t.\n\u2022 We test multiple benchmarks (see Figures 2 and 3),\ndemonstrating competitive throughput and runtime with\nstatistical guarantees on collision avoidance.\nRelated Work\nTraditional MAPF is focused on static environments in a\none-shot setting producing a plan for all agents before ex-\necuting any actions (Stern et al. 2019). Finding a solution of\ncollision-free paths for all agents is an NP-hard problem, yet\npopular solvers such as Conflict-Based Search (CBS) and\nits extensions scale in the number of conflicts rather than\nagents (Sharon et al. 2015; Barer et al. 2014). Recent work\nin MAPF has shifted to address issues of scalability, robust-\nness to delays or failures, and lifelong scenarios (Stern et al.\n2019; Strawn and Ayanian 2022; Bellusci et al. 2020; Ma\net al. 2017; Wan et al. 2018). In lifelong MAPF agents at-\ntend to a sequence of goals over an indefinite time horizon,\nplanning as they take actions. Most solutions split the life-\nlong problem into multiple MAPF instances and iteratively\nresolve conflicts (Li et al. 2021; Ma et al. 2017).arXiv:2507.22282v1  [cs.MA]  29 Jul 2025\n\n--- Page 2 ---\nFigure 2: Three MAPF DUA warehouse instances (small,\nmedium, large) with 6 controlled agents (blue) and 4 dy-\nnamic agents (red).\nExisting MAPF strategies can incorporate real-time up-\ndates, allowing controlled agents to react to new infor-\nmation about obstacles or agent locations. Closest to our\nwork is probabilistic MAPF, where delays and agent failures\nare accounted for with stochastic models and robust plans\nthat are generated to account for uncertainty of controlled\nagents (Atzmon et al. 2018, 2020; Okumura and Tixeuil\n2023; Chen et al. 2021; Phan et al. 2024). However, these\nare computationally expensive, lack statistical safety guar-\nantees, have limited time horizons, focus on agent failures\nor changing environments, and do not consider planning\naround dynamic uncontrollable agents in the environment.\nModel Predictive Control (MPC) applies actions in a re-\nceding horizon, selecting a minimum-cost action sequence\nrepeatedly using predictions of dynamic agents conditioned\non the current state and a history of states seen so far to im-\nprove motion planning and control (Rawlings 2000). How-\never, models of uncontrollable agents are usually not avail-\nable or exactly known. Inspired by predictive control algo-\nrithms and recent work in uncertainty quantification of tra-\njectory predictors with conformal prediction (CP), see e.g.,\n(Angelopoulos, Barber, and Bates 2024; Lindemann et al.\n2023; Stankeviciute, M Alaa, and van der Schaar 2021),\nwe develop CP-solver as an extension of ECBS to address\nMAPF among uncontrollable agents. Alternative approaches\nmodel the underlying uncertainty as a Gaussian distribution,\nuse Kalman filters, or use reachability analysis (Berkenkamp\nand Schoellig 2015; Thrun 2002; Ames et al. 2019; Rober\net al. 2023; Bansal et al. 2017). These approaches can en-\nsure safety, but are often overly conservative, computation-\nally expensive, or make unrealistic assumptions.\nPreliminaries: Multi-Agent Path Finding\nLetA:={1,2, . . . , n }be a set of ncontrollable agents\nmoving on a given undirected and finite graph G:= (V, E),\nwhere V\u2286RNcorresponds to a set of reachable ver-\ntices, such as locations in an N:= 2 dimensional grid,\nand(Bt,a,Bt+1,a)\u2208E\u2286RN\u00d7RNare edges connect-\ning two adjacent vertices traversed by an agent a\u2208Aat\ntimet\u2208[0, T\u22121]where T >0is a final mission horizon.\nEach edge has a non-negative cost for traversing from Bt,a\ntoBt+1,a, denoted by a given cost((Bt,a,Bt+1,a))\u2208[1,\u221e).\nAll agents, a\u2208A, are assigned (random or user-specified)\nstarting locations \u03c8s,a\u2208Vand goal locations \u03c8g,a\u2208\nV, written as an agent\u2019s assignment \u03c8a:= (\u03c8s,a, \u03c8g,a).\nAgents select edges to move to a neighboring vertex or\nwait at the current vertex. Agents wait at their goal un-til all goals are achieved. All vertices hence have an edge\nback to themselves. To move between vertices, an agent\ntakes an action at timestep t, moving from vertex Bt,a\u2208\nVwith \u03c0a:V\u00d7T\u00d7A\u2192Vto their next vertex\nBt+1,a:=\u03c0a(Bt,a, t, a). To simplify notation, we will use\n\u03c0a(Bt,a)and thereby suppress the input arguments of the\ntimestep tand the agent a. The solver has access to the\ngraph, agents, and timestep to generate a sequence of actions\n\u03c00:T\u22121,a:={\u03c0a(B0,a), . . . , \u03c0 a(BT\u22121,a)}that produce a se-\nquence of vertices B0:T,a:={B0,a,B1,a, . . . ,BT,a}, where\nB0,a:=\u03c8s,a,BT,a:=\u03c8g,a,Bt,a\u2208V,\u2200t\u2208[0, T\u22121]and\n(Bt,a,Bt+1,a)\u2208E,\u2200t\u2208[0, T\u22121].\nA collision at timestep t, denoted by K(t,a,b):= 1, exists\nif a vertex conflict, denoted by Kt,(Bt,a,Bt,b), or edge con-\nflict, denoted by K(Bt,a,Bt+1,a),(Bt,b,Bt+1,b), exists where two\nagents a, b\u2208A, a\u0338=boccupy the same location at the same\ntime, i.e., Bt,a=Bt,b, or traverse the same edge in oppo-\nsite directions, i.e., (Bt,a,Bt+1,a) = (Bt+1,b,Bt,b), respec-\ntively. Two trajectories are conflict-free if for all timesteps\nthey contain no conflicts: K(t,a,b):= 0,\u2200t\u2208[0, T].\nA solver is valid if the joint solution \u03c00:T\u22121contains ac-\ntion sequences \u03c00:T\u22121,athat produce a conflict-free trajec-\ntory from start to goal, B0:T,a, for all agents a\u2208A. A so-\nlution \u03c00:T\u22121is optimal if it is valid and minimizes a pre-\ndefined cost:\nmin\n\u03c00:T\u22121nX\na=1cost(\u03c0a(Bt,a))\u2200t\u2208[0, T\u22121] (1)\nwhere cost(\u03c0a(Bt,a)) := cost((Bt,a,Bt+1,a)). The service\ntime, denoted by S(B0:T,a) :=|B0:T,a|, is the number of\nedges in the trajectory. If all edge costs are equal to one, thenPT\u22121\nt=0cost(\u03c0a(Bt,a)) =S(B0:T,a). We note that this is not\nnecessarily equivalent to the makespan, which is the time T\nwhen all agents have reached their goals. Solutions are often\nevaluated by their makespan, service time, and runtime (the\ntime it takes to compute the solution in seconds).\nConflict-Based Search (CBS), a popular MAPF algo-\nrithm, finds a valid conflict-free solution \u03c00:T\u22121(Sharon\net al. 2015) by splitting the problem into a high- and low-\nlevel search. Using A* to find the shortest path between\na start and goal state at the low-level (Hart, Nilsson, and\nRaphael 1968) and building a constraint tree at the high\nlevel. A new node on the tree is added for the first con-\nflict found, represented as (a,Bt,a, t), forcing the agent a\nto avoid the vertex Bt,aat timestep t. CBS selects the lowest\ncost solution seen so far, until a solution with no conflicts\nis found. This guarantees an optimal solution in terms of\nmakespan. However, the search grows exponentially and be-\ncomes intractable with runtime limits (Sharon et al. 2015).\nECBS is a bounded suboptimal variant based on a focal\nsearch, inflating the low-level search heuristic by wwhile\nmaintaining an additional focal search priority queue at the\nhigh-level. The bound wdetermines how much the resulting\nsolution cost may vary from the optimal solution, speeding\nup the search significantly (Barer et al. 2014).\n\n--- Page 3 ---\nProblem Formulation: MAPF DUA\nWe define the Multi-Agent Path Finding (MAPF) among\nDynamic Uncontrolled Agents (DUA) problem as desiring\na set of collision-free action sequences for the set of con-\ntrollable agents from start to goal locations among a set\nof dynamic and uncontrollable agents. Therefore, let U:=\n{1,2, . . . , m }be a set of muncontrolled dynamic agents.\nAll agents in AandUhave a starting location and a goal\nlocation, denoted by \u03c8a,\u2200a\u2208Aand\u02dc\u03c8b,\u2200b\u2208U. We\nseek a valid sequence of actions \u03c0a,\u2200a\u2208Asuch that the\nsolver generates paths for each controlled agent from \u03c8s,ato\n\u03c8g,awhile minimizing service time and avoiding collisions\n(K(t,a,b)\u0338= 1), with uncontrolled agents controlled by an\nunknown action sequence \u02dc\u03c0. We write this one-shot, open-\nloop problem as an ideal optimization:\nmin\n\u03c00:T\u22121nX\na=1(S(Ba)) (2a)\nsubject to\nB0,a:=\u03c8s,a,\u2200a\u2208A (2b)\nBt+1,a:=\u03c0a(Bt,a),\u2200a\u2208A (2c)\nB1:\u02c6T,a:={B1,a, . . . ,BT\u22121,a, \u03c8g,a},\u2200a\u2208A (2d)\n\u02dcB0,b:=\u02dc\u03c8s,b,\u2200b\u2208U (2e)\n\u02dcBt+1,b:= \u02dc\u03c0b(\u02dcBt,b),\u2200b\u2208U (2f)\n\u02dcB1:\u02c6T,b:={\u02dcB1,b, . . . , \u02dcBT\u22121,b,\u02dc\u03c8g,b},\u2200b\u2208U (2g)\nK(t,a,k )\u0338= 1,\u2200t\u2208[0, T],\u2200(a, k)\u2208A\u00d7A (2h)\nK(t,a,b)\u0338= 1,\u2200t\u2208[0, T],\u2200a\u2208A,\u2200b\u2208U (2i)\nSolving this problem requires knowledge of the assignment\n(\u02dc\u03c8s,b,\u02dc\u03c8g,b)of uncontrolled agents and their action sequence\n\u02dc\u03c0t,b, which are not available. In general, the open-loop (one-\nshot) problem becomes intractable as the graph size, agent\nset, and minimum makespan increase (Stern et al. 2019).\nIn lifelong MAPF all agents in AandUhave a starting\nlocation and a qa>0length sequence of locations to attend\nto sequentially, \u03c8ga:={\u03c8g1a, \u03c8g2a, . . . , \u03c8 gqa},\u2200a\u2208Aand\n\u02dc\u03c8gb:={\u02dc\u03c8g1b,\u02dc\u03c8g1b, . . . , \u02dc\u03c8gqb,b},\u2200b\u2208U. In the closed-loop\n(lifelong) DUA problem, we change the minimization of ser-\nvice time to maximization of throughput:1\n\u02c6T\u0393(B0:\u02c6T,a)where\n\u0393(B0:\u02c6T,a)is the number of goals a trajectory achieves over\na user-set long time horizon \u02c6T > 0. We write this lifelong,\nclosed-loop problem (L-MAPF DUA) as an ideal optimiza-\ntion:\nmax\n\u03c00:\u02c6T\u221211\n\u02c6TnX\na=1(\u0393(Ba)) (3a)\nsubject to\nB0,a:=\u03c8s,a,\u2200a\u2208A (3b)\nBt+1,a:=\u03c0a(Bt,a),\u2200a\u2208A (3c)\nB1:\u02c6T,a:={B1,a, . . . , \u03c8 g1a, . . . ,B\u02c6T\u22121,a, \u03c8gqa},\u2200a\u2208A\n(3d)\n\u02dcB0,b:=\u02dc\u03c8s,b,\u2200b\u2208U (3e)\u02dcBt+1,b:= \u02dc\u03c0b(\u02dcBt,b),\u2200b\u2208U (3f)\n\u02dcB1:\u02c6T,b:={\u02dcB1,b, . . . , \u02dc\u03c8g1b, . . . , \u02dcB\u02c6T\u22121,b,\u02dc\u03c8gqb},\u2200b\u2208U\n(3g)\nK(t,a,k )\u0338= 1,\u2200t\u2208[0,\u02c6T],\u2200(a, k)\u2208A\u00d7A (3h)\nK(t,a,b)\u0338= 1,\u2200t\u2208[0,\u02c6T],\u2200a\u2208A,\u2200b\u2208U (3i)\nApproach: CP-Solver\nThe uncontrolled agents\u2019 action sequences and assignments\nin equations (2e)-(2g) and (3e)-(3g) are typically unknown,\ne.g., in the case of a human agent. Thus, it is impossible to\nsolve the DUA problem exactly. We instead focus on an ap-\nproximate solution where we aim for probabilistic collision\navoidance by replacing equations (2i) and (3i) with:\nProb(K(t,a,b)\u0338= 1,\u2200t\u2208[0,\u00afT],\u2200a\u2208A,\u2200b\u2208U)\u22651\u2212\u03b4\n(4)\nwhere \u00afT\u2208 {T,\u02c6T}and\u03b4\u2208(0,1)is a user defined failure\nprobability. To solve this approximate problem, we advocate\nfor an approach that (1) predicts the paths of dynamic agents,\nand (2) quantifies prediction uncertainty statistically.\nTrajectory Prediction\nLetD\u02dcBbe an unknown distribution over uncontrolled dy-\nnamic agent trajectories. In this case, the random trajectory\n(\u02dcB0,\u02dcB1,\u00b7\u00b7\u00b7)\u223c D \u02dcBis generated by \u02dcBt+1:= \u02dc\u03c0(\u02dcBt)where\nthe stacked agent states \u02dcBt:= (\u02dcBt,1,\u00b7\u00b7\u00b7,\u02dcBt,m)at time tare\ndrawn from RNm. We make no assumptions on the form of\nthe distribution D\u02dcBbut assume 1) the availability of data in-\ndependently drawn from D\u02dcBgenerated by \u02dcBt+1:= \u02dc\u03c0(\u02dcBt),\nand 2) that D\u02dcBis independent of any controlled agents.\nAssumption 1 We have a dataset of trajectories D:=\n{\u02dcB(1),\u00b7\u00b7\u00b7,\u02dcB(d)}in which each of the dtrajectories \u02dcB(i):=\n{\u02dcB(i)\n0,\u02dcB(i)\n1,\u00b7\u00b7\u00b7} is independently drawn from D\u02dcB.\nAssumption 1 is generally not restrictive, especially in\nwarehouse environments, as data can be recorded before de-\nployment or obtained from rapidly advancing high-fidelity\nsimulators and open datasets (Padalkar et al. 2023). We split\nthe dataset of trajectories Dinto separate training datasets\nDtrain ,Dval, and Dtestfrom which we will train, validate,\nand test a trajectory predictor. Additionally, setting aside a\nsmall dataset Dcalto quantify prediction uncertainty.\nAssumption 2 For any time t\u22650, the controlled agent se-\nquences (\u03c0a(B0,a),\u00b7\u00b7\u00b7, \u03c0a(Bt\u22121,a)),\u2200a\u2208Aand the re-\nsulting trajectories (B0,\u00b7\u00b7\u00b7,Bt), do not change the distri-\nbution of dynamic agent trajectories (\u02dcB0,\u02dcB1,\u00b7\u00b7\u00b7)\u223c D \u02dcB.\nAssumption 2 holds approximately in many applica-\ntions, e.g., when controlled agents operate in socially ac-\nceptable ways without changing the intentions of other\nagents. In our experiments, the dynamic uncontrollable\nagents are given the right of way and are not influenced\nby controlled agents. Furthermore, it has been shown that\nCP guarantees remain valid even under small distribution\nshifts (Cauchois et al. 2023; Strawn, Ayanian, and Linde-\nmann 2023). In cases where such interaction is present,\n\n--- Page 4 ---\nrobust uncertainty quantification could help preserve guar-\nantees (Angelopoulos, Barber, and Bates 2024; Cauchois\net al. 2023). Given a prediction interval \u03bb:= [1, H], where\nH\u2208[0,\u221e)is a prediction horizon, and the history of dy-\nnamic agent observations \u02dcB0:t, we seek a trajectory predic-\ntorY:R(t+1)Nm\u2192RNmHthat predicts the Hfuture\nagent vertices (\u02dcBt+1, . . . , \u02dcBt+H)as\u00afB\u03bb:=Y(\u02dcB0:t)where\n\u00afB\u03bb:= ( \u00afBt+1,\u00b7\u00b7\u00b7,\u00afBt+H). In principle, we can use any\ntrajectory predictor Y, e.g., long short-term memory net-\nworks (Alahi et al. 2016; Hochreiter 1997) or transformer ar-\nchitectures (Nayakanti et al. 2022). We independently sam-\nple the training dataset Dtrain fromD\u02dcBwith trajectories of\nlength T, where \u02dcB(i)\n0:T:= ( \u02dcB(i)\n0,\u00b7\u00b7\u00b7,\u02dcB(i)\nt,\u02dcB(i)\nt+1,\u00b7\u00b7\u00b7,\u02dcB(i)\nT)\nis the i-th trajectory in the dataset. We train the predictor by\nminimizing the following loss function over Dtrain , validat-\ning with Dvaland testing accuracy with Dtest:\nmin\nY1\n|Dtrain||Dtrain|X\ni=1\u2225\u02dcB(i)\n\u03bb\u2212Y(\u02dcB(i)\n0:t)\u22252. (5)\nUncertainty Quantification of Predictions\nWe use conformal prediction (CP) to construct statistical re-\ngions around the predicted trajectories that contain the true,\nyet unknown, agent trajectory with a user-defined confi-\ndence level (Angelopoulos and Bates 2021; Lindemann et al.\n2024). The CP method we use constructs valid prediction re-\ngions for any learned time series predictor Y.\nIn Algorithm 1, we present a modified version of a re-\ncent framework from (Cleaveland et al. 2024) that uses lin-\near complementarity programming (LCP) to obtain tight tra-\njectory prediction regions by compensating for prediction\nerrors across timesteps. In contrast to (Cleaveland et al.\n2024), we incorporate reasoning over multiple uncontrol-\nlable agents. In LCP CP, normalization constants \u03b1t+hare\nintroduced to normalize prediction errors at each time step,\nresulting in less conservative prediction regions compared to\nother existing work. We summarize Algorithm 1 next.\nGiven a failure probability \u03b4\u2208(0,1), a history of ob-\nservations \u02dcB0:t:= ( \u02dcB0,\u00b7\u00b7\u00b7,\u02dcBt)at time tfor all mdy-\nnamic agents \u02dcBt:= ( \u02dcBt,0,\u00b7\u00b7\u00b7,\u02dcBt,m), and trajectory pre-\ndictor Yproducing predictions \u00afB\u03bb:= ( \u00afBt+1,\u00b7\u00b7\u00b7,\u00afBt+H)\nfor the specified prediction horizon \u03bb, Algorithm 1 gener-\nates values C\u03bb:= (Ct+1,\u00b7\u00b7\u00b7, Ct+H)as prediction intervals\naround each prediction that guarantee:\nProb(R(\u00afBt+h,b,\u02dcBt+h,b)\u2264Ct+h,\u2200b\u2208U,\u2200h\u2208\u03bb)\u22651\u2212\u03b4\n(6)\nwhere R(\u00afBt+h,\u02dcBt+h)is the prediction error:\nR(\u00afBt+h,\u02dcBt+h) :=\u2225\u00afBt+h\u2212\u02dcBt+h\u2225. (7)\nWe start the CP process by first computing the max pre-\ndiction error \u00afC(i)\nt+h:= max\nb\u2208U(R(\u00afB(i)\nt+h,b,\u02dcB(i)\nt+h,b))at each\ntimestep across instances in the calibration dataset Dcal\n(lines 2-5). Then, we split the prediction errors \u00afC\u03bbinto\nC\u03bb,cal 1andC\u03bb,cal 2. Next, we follow two key steps:\n1. Computing normalization constants \u03b1t+h\u22650,\u2200h\u2208\u03bb\nwithC\u03bb,cal 1according to (Cleaveland et al. 2024), re-\nferred to as the routine LCP (\u00b7)(line 7).2. Taking the maximum across timesteps of the values\n\u02c6C(i)\ncal2:= max( \u03b1t+1C(i)\nt+1,cal2, . . . , \u03b1 t+HC(i)\nt+H,cal 2)and\nsorting them in non-decreasing order (lines 8-10).\nAfterwards, we append infinity as the (|Dcal,2|+1)-th value\n(line 11) before setting Ct+has the p:=\u2308(|Dcal,2|+1)(1 \u2212\n\u03b4)\u2309-th smallest value of \u02c6C(i)\ncal2divided by \u03b1t+h(lines 12-14).\nThe produced CP intervals Ct+hare guaranteed to satisfy\n(6) and are later passed to Algorithm 2 within CP-Solver.\nAlgorithm 1: Conformal Prediction Setup\nInput: Datasets (Dtrain, Dval, Dtest, Dcal),\nConfidence \u03b4, Horizon \u03bb, Timesteps (t, T)\nOutput: Predictor Y, CP Intervals C\u03bb\n1Learn Predictor Yfrom Dtrain, Dval, Dtest(Eq. 5);\n2foriinDcaldo\n3 \u00afB(i)\n\u03bb\u2190Y(\u02dcB(i)\n0:t);\n4 forh\u2208[1, H]do\n5 Compute \u00afC(i)\nt+h\u2190max\nb\u2208U(R(\u00afB(i)\nt+h,b,\u02dcB(i)\nt+h,b));\n6\u00afC\u03bb,cal 1,\u00afC\u03bb,cal 2\u2190Split \u00afC\u03bb;\n7{\u03b1t+1, . . . , \u03b1 t+H} \u2190 LCP(\u00afC\u03bb,cal 1, \u03b4);\n8foriin\u00afC\u03bb,cal 2do\n9 \u02c6C(i)\ncal2\u2190\nmax( \u03b1t+1\u00afC(i)\nt+1,cal2, . . . , \u03b1 t+H\u00afC(i)\nt+H,cal 2);\n10Sort\u02c6Ccal2in non-decreasing order;\n11Append \u02c6C(|Dcal,2|+1)\ncal2\u2190 \u221e ;\n12Setp\u2190 \u2308(|Dcal,2|+ 1)(1 \u2212\u03b4))\u2309;\n13forh\u2208\u03bbdo\n14 SetCt+h\u2190\u02c6C(p)\ncal2\n\u03b1t+h;\nOpen-Loop CP-Solver\nIn Algorithm 2, we present our Conformal Predictive Solver\n(CP-Solver). Before planning a path for the controlled\nrobots, we run Algorithm 1 to obtain CP intervals C\u03bb:=\n{C\u03bb,1, . . . , C \u03bb,m}. During planning, i.e., at runtime t, we\nhave access to the observations \u02dcB0:tof dynamic agents from\nwhich we compute predictions \u00afB\u03bb:=Y(\u02dcB0:t)(lines 1-2).\nWe first project our CP intervals satisfying equation (6)\nonto the discrete graph Gby building CP interval sets. This\nis a crucial step in our method that is needed since the CP\nintervals define a region in RN, while MAPF operates on\ndiscrete graphs G. Our method builds a set of CP interval\nvertices at every timestep: \u00afB\u03bb,C:={\u00afBt+1,C, . . . , \u00afBt+H,C}\nvia the discretize (\u00b7)function (line 3). Specifically, the set\n\u00afBt+h,C:={v0, v1, . . . , v \u03c1t+h}contains \u03c1t+hvertices vj\u2208\nVthat satisfy: 1) \u2225\u00afBt+h,b\u2212vj\u2225\u2264Ct+hfor some agent\nb\u2208U, i.e., vjisCt+h-close to the predictions of an uncon-\ntrollable agent b, and 2) the shortest path is SP(\u02dcBt,b, vj)\u2264\nhso that vjcan be reached from \u02dcBt,bin less than hedges.\nIf the controllable agents in Aavoid the CP interval vertex\n\n--- Page 5 ---\nsets\u00afB\u03bb,ConG, it follows that collisions with uncontrollable\nagents are avoided with a probability of no less than 1\u2212\u03b4.\nWe begin our modified ECBS algorithm with an ini-\ntial node Zand an empty set of constraints (line 4). The\nhigh-level node stores: the constraints on trajectories for\neach controlled agent, predicted trajectories of uncontrolled\nagents, discretized CP interval sets, and controlled agent\npaths. These controlled agent paths are found with A* in\nthe low level(\u00b7)search and stored as action sequences into\nZ\u03c0t:T\u22121(lines 5-6). The cost of the trajectories, Zcost, is the\nservice time for each controlled agent trajectory (line 7). The\nnode Zis added to a minimum cost-based priority queue,\nwhich our modified ECBS solver searches over (line 8).\nAlgorithm 2: Open-Loop CP-Solver\nInput: Graph G, Agents A, Dynamic Agents U,\nAssignments \u03c8, Horizon \u03bb, Trained Predictor\nY, Conformal Intervals C\u03bb, timestep t\nOutput: Solution Z\u03c0t:T\u22121and Trajectories Bt+1:t+H\n1Set\u02dcB0:t\u2190observe (U, t);\n2Set\u00afB\u03bb\u2190Y(\u02dcB0:t);\n3Set\u00afB\u03bb,C\u2190discretize (C\u03bb,\u00afB\u03bb)\n4SetZconstraints \u2190 \u2205;\n5SetB0:T\u2190lowlevel(Zconstraints , G, A, \u03c8 );\n6SetZ\u03c0t:T\u22121\u2190\u00afB\u03bb,C\u222a\u00afB\u03bb\u222a B0:T;\n7SetZcost\u2190Pn\ni=0S(B0:T,i);\n8Insert OPEN \u2190Zinto the priority queue;\n9while OPEN not empty do\n10 SetZ\u2190minimum Zcostnode from OPEN;\n11 GetKt,a,b\u2190first conflict (Z\u03c0t:T\u22121);\n12 ifKt,a,bis 0then\n13 break ;\n14 ifagent a, binUthen\n15 Set\u03d5\u2190\u2205;\n16 else if agent ainAandbinUthen\n17 Set\u03d5\u2190(a);\n18 else if agent ainUandbinAthen\n19 \u03d5\u2190(b);\n20 else\n21 \u03d5\u2190(a, b);\n22 forrin\u03d5do\n23 SetZr\u2190new node(Z);\n24 SetZconstraints ,r\u2190(Kt,a,b, r);\n25 SetB0:T,r\u2190\nlowlevel(Zconstraints ,r, G, r, \u03c8 r);\n26 SetZ\u03c0t:T\u22121,r\u2190update solution (B0:T,r);\n27 SetZcost,r\u2190Zcost\u2212Zcost,r +S(B0:T,r);\n28 Insert OPEN \u2190Zr;\n29SetBt+1:t+H\u2190Z\u03c0t:T\u22121(Bt:T\u22121);\nWe continue by selecting the minimum cost node from\nOPEN until OPEN is empty (lines 9-10). Then, each\nnode Zis checked for a conflict between pairs of agents,\npredicted paths, and CP interval vertices (line 11). We donot add a constraint to the set of constraints, denoted by \u03d5, if\nthe conflict involves two uncontrolled agents (line 14). If the\nconflict involves one uncontrolled agent or CP interval ver-\ntex, only the controlled agent receives a constraint (lines 16-\n19). If the conflict involves both uncontrolled agents, both\nreceive a constraint (line 21). We then iterate over the found\nconstraint\u2019s agents (line 22) to build a new node Zpfor\neachp\u2208\u03d5that inherits from the current node Zthrough\nnew node(\u00b7)(line 23) and add it to the priority queue (line\n28). If a conflict-free solution is found (line 13), we execute\nthe action sequence \u03c0t:T\u22121for all controlled agents, avoid-\ning the CP interval sets \u00afB\u03bb,C. Thus, equation (6) is satisfied,\nand our algorithm contributes a probabilistically safe solu-\ntion to the one-shot DUA problem.\nCorollary 1 Algorithm 1 guarantees that equation (6)is\nsatisfied. Then, by construction, a valid solution \u03c00:T\u22121for\ncontrolled agents produced by CP-Solver in Algorithm 2\nguarantees that collision as per equations (2h)-(2i) are\navoided with the user-specified confidence level of 1\u2212\u03b4.\nClosed-Loop CP-Solver\nIn the open-loop method, the plan is produced and then ex-\necuted until all agents reach their goals. Next, we adapt\nCP-Solver to solve lifelong MAPF DUA (L-MAPF DUA)\nby adopting a rolling-horizon conflict resolution (RHCR)\nframework (Li et al. 2021). To do so, we select an ECBS\nsolver conflict horizon, denoted by \u02c6w, and a replanning\nwindow, denoted by H, where H\u2264\u02c6w. In RHCR, con-\nflicts are resolved up to this maximum timestep \u02c6wand con-\ntrolled agents replan their paths every Htimesteps (Li et al.\n2021). In our implementation, we set the prediction hori-\nzon as \u03bb:= [t+ 1, t+H]using the replanning horizon H.\nWhile effective in practice, we note that RHCR algorithms\nare in general incomplete. The RHCR framework may over-\nlook long-term conflicts, occasionally deadlocking. We ex-\ntend our CP-Solver algorithm by iteratively solving multi-\npleH-windowed open-loop instances, updating our obser-\nvations of the dynamic agents at the end of each windowed\nplanning horizon. We summarize our algorithm next.\nBefore planning and executing a user-defined \u02c6Ttotal\ntimesteps, we train the predictor and run the CP process as\ndone in Algorithm 1. Our closed-loop approach then tracks\nthe current timestep t\u2208 {0, H,2H,\u00b7\u00b7\u00b7,\u02c6T], observing the\nstates of the dynamic agents every Hsteps, and stores their\nlocations in the history of observations \u02dcB0:t. We then run Al-\ngorithm 2 as a windowed instance of the open-loop approach\nand in a rolling-horizon fashion. Here, observations are in-\nput to the predictor to produce predictions \u00afB\u03bbthat are then\nused to generate discretized CP interval sets \u00afB\u03bb,Cfor all dy-\nnamic agents in Uwith discretize (\u00b7). Once a conflict-free so-\nlution is found by Algorithm 2, we apply the action sequence\n\u03c0a(Bh,a),\u2200a\u2208A,\u2200h\u2208[t, t+H\u22121], update t:=t+H,\nand repeat while t < \u02c6T. Agents whose shortest path on G\nto their current sequence of goals is less than the replanning\nhorizon: SP(Bt,a, \u03c8a)\u2264H, are randomly assigned addi-\ntional goal locations to visit. At the end of each executed\nwindow, we observe the uncontrolled agent movements and\nsolve the next H-windowed MAPF DUA instance.\n\n--- Page 6 ---\nSolvability : Not all MAPF instances (composed of a\ngraph G, controlled agents A, and task assignments \u03c8) are\nsolvable (Sharon et al. 2015). A sufficient condition for solv-\nability (feasible to produce a solution) is that the instance is\nwell-formed (Ma et al. 2017). In traditional MAPF, an in-\nstance is well-formed if and only if the number of tasks is\nfinite, there are exactly as many or more task spots (avail-\nable vertices, g\u2208V, for goals) as the number of agents, and\nfor any assignment there exists a path between the start and\ngoal vertices that does not cross another goal vertex. How-\never, in lifelong settings, these conditions may not hold with\nnew tasks arriving over an infinite mission horizon. To ad-\ndress this, it is common to introduce assumptions that main-\ntain the well-formed condition for lifelong MAPF, such as\nreserving goal vertices (Ma et al. 2017). In L-MAPF DUA,\ndynamic uncontrolled agents may break the well-formed as-\nsumption, e.g., an uncontrolled agent stops and prevents an-\nother vertex from being reached. To preserve solvability, we\nintroduce the following three-part assumption.\nAssumption 3 First, we reserve the assigned goal vertices,\ni.e., all goal assignments, \u03c8g,afor all a\u2208A, are in desig-\nnated task spots that only the assigned agent can enter after\nmoving out of their starting vertex. Second, if the controlled\nagent cannot find a path to its goal, the agent can remain\nwhere it is and be excluded from the input to our modified\nECBS solver. Third, we assume all uncontrolled agents will\neventually reach an assigned goal vertex or keep moving.\nWe maintain the well-formed attribute by allowing a con-\ntrolled agent to pause and an uncontrolled agent to proceed\nwith the next planning window if needed to prevent a dead-\nlock. We note that these interactions are undesirable and will\nbe counted as collisions during the experiments.\nReal-time Applicability : Real-world L-MAPF DUA\nwith windowed horizons requires computing and execut-\ning actions before uncontrolled agents physically move be-\nyond the given window. This is also true for traditional\nMAPF, which assumes agents\u2019 solutions can be computed\nfast enough for real-world applications. To ensure real-time\napplicability, we make the following assumptions. First,\nall agents begin and complete their action sequences dur-\ning the execution window [t, t+H\u22121]. At each transi-\ntion, observations of the dynamic agents are obtained, \u02dcB0:t,\nand solutions \u03c0t,t+H\u22121for the next horizon are produced\nbefore the following execution window begins. Second, un-\ncontrolled agents do not move across more than one edge\nduring a timestep transition [t, t+ 1] . ECBS and RHCR\nwere selected to prioritize speed in re-planning and uphold\nthese assumptions. Future work could explore ECBS alter-\nnatives, calibrate the time horizons to decrease runtime fur-\nther, or integrate CP-Solver with continuous-space forms of\nMAPF (Andreychuk et al. 2022; H \u00a8onig et al. 2016).\nExperimental Evaluation\nWe conducted a series of experiments (on a 5.0 GHz Intel\nCore i9-9900K computer with 16GB RAM) across bench-\nmark maps (Stern et al. 2019), with multiple parameter con-\nfigurations. We ran a test suite with permutations of the three\nwarehouse-like maps (small, medium, and large shown in\nFigure 3: Game maps: (left) den201d and (right) arena.\nMap Small Normal Large Arena Den201d\nRate 0.978 0.987 0.962 0.981 0.984\nTable 1: CP coverage of trajectory prediction error across\n100 open-loop instances per map, with \u03b4= 0.05.\nFigure 2), two video game maps (shown in Figure 3), and pa-\nrameters: controlled agent set sizes of [5,10,20,30,40,50],\nuncontrolled agent set sizes of [1,3,5,10,20,30,40], con-\nformal confidence level \u03b4:={0.01,0.05}, conflict hori-\nzon\u02c6w:={1,3,5,10,15}and replanning window H:=\n{1,3,5,10,15}for which H\u2264\u02c6w.\nWe compare our approach against three other algorithms.\nFirst, IGNORE-ECBS, which uses the standard ECBS\nsolver with no knowledge of dynamic agents to produce a\nbaseline (Barer et al. 2014). Second, OBSTACLE-ECBS,\nsensing the current locations of agents and treating them\nas static obstacles for the entire planning window. Third,\nPRED-ECBS, a version of CP-Solver that uses predictions,\nbut not CP intervals. Results were averaged across three ran-\ndom initializations of agent starting positions and goal as-\nsignments for each permutation of configuration parameters\nto account for variability in initial placements. We recorded\nseveral performance metrics for each test run: 1) through-\nput for goal attainment performance, 2) runtime for solver\nfeasibility, and 3) number of collisions with uncontrolled\nagents for safety. We note that collisions are counted at each\ntimestep; if two agents remain in a collision, they will in-\ncrease the collision count at every timestep.\nWe collected 5,000 trajectories of dynamic agents for\neach map and dataset type operating with A* paths to ran-\ndom goals. We trained a simple LSTM network for each\nmap type and size, setting the architecture to be 2layers\nof128hidden units. Our model uses a history of length 4\nwith various prediction horizons of (1,3,5,10)timesteps to\npredict the path of dynamic agents. We assign the closest\nunassigned task spot to the final prediction as the goal ver-\ntex and assume the agent stays in this goal location. We set\nthe ECBS suboptimality bound to 1.5, aligned with the lib-\nMultiRobotPlanning library (H \u00a8onig et al. 2016). This library\nimplements popular task and path planning algorithms. We\nhave modified their version of ECBS and increment the focal\nweight by 1after a 100-second timeout.\nResults\nWe illustrate the closed-loop results in Figure 4.\nProbabilistic Collision Avoidance : Corollary 1 ensures\nsafe planning around dynamic agents at a user-defined con-\nfidence level. With \u03b4= 0.05, Table 1 displays the results of\n\n--- Page 7 ---\nFigure 4: Closed-loop CP-Solver averaged across all agent initialization, maps, and configuration parameters not specified:\n(upper-left) all maps, (upper-right) warehouse maps, (lower-right) video game maps, (lower-left) CP-Solver.\nUncontrolled Agents 1 3 5\nSafety Violations 0/100 0/100 2/100\nTable 2: Number of the 100open-loop CP-Solver runs that\nhave one or more collisions (a safety violation) with param-\neters: #controlled := 10 ,\u03b4:= 0.05, and H:= 15 .\nUncontrolled Agents 1 3 5\nSafety Violations 1/100 3/100 4/100\nTable 3: Number of the 100closed-loop CP-Solver runs that\nhave one or more collisions (a safety violation) with param-\neters: #controlled := 10 ,\u03b4:= 0.05,H:= 10 , and \u02c6T:= 100 .\nrunning our open-loop formulation across all maps and pa-\nrameters, confirming that our CP intervals capture 95% of\nprediction errors in the prediction window. In Tables 2 and\n3, we show that the number of CP-Solver runs, for a selec-\ntion of parameters and agents, that have a safety violation\n(one or more collisions) is less than the expected \u03b4bound\naligning with Corollary 1. Assumption 2 acknowledges CP\u2019s\nvulnerability to distribution shifts, although the theoretical\nguarantees hold under small shifts (Cauchois et al. 2023;\nStrawn, Ayanian, and Lindemann 2023) and robust CP ex-\ntensions exist for larger shifts (Angelopoulos, Barber, and\nBates 2024; Cauchois et al. 2023).\nWe note that OBSTACLE-ECBS produces fewer colli-\nsions on average; however, this is due to the increased num-\nber of obstacles that deadlock areas of the map, forcing\nagents to wait or take alternative paths. Differences in map\nand agent configurations can significantly impact the perfor-\nmance of OBSTACLE-ECBS. CP-Solver offers guarantees\nfor reliable performance and safety, using predictions to en-\nable more agents to keep moving in the direction of theirgoal, which may bring them closer to uncontrolled agents\nthan OBSTACLE-ECBS.\nThroughput : CP-Solver achieves higher throughput than\nOBSTACLE, closely trailing PRED (lacks guarantees) and\nIGNORE (disregards dynamic agents entirely).\nRuntime : CP-Solver runtime remains competitive,\nthough closed-loop runs occasionally exhibit higher out-\nliers. Some initializations lead to increased runtimes, as CP-\nSolver retains more agents in the solver while IGNORE and\nOBSTACLE exclude more agents. Future work could ex-\nplore ECBS node expansion and runtime optimizations.\nHorizon Parameters : As \u02c6wandHincrease, collisions\ndecrease slightly while throughput and runtime increase.\nCP-Solver throughput decreases and runtime increases at\nlarge Hvalues, consistent with findings that extended\nhorizons in RHCR do not always improve overall perfor-\nmance (Li et al. 2021). Improving the predictor has a more\nsignificant impact on performance than greatly increasing\nprediction length.\nConclusion\nPresenting the MAPF DUA problem and CP-Solver broad-\nens the scope of CBS-based solvers and provides statistical\nsafety guarantees for MAPF applications in environments\nwith uncontrolled dynamic agents. We have contributed CP-\nSolver, which reduces collisions compared to standard ap-\nproaches and remains competitive in terms of throughput,\nnumber of collisions, and runtime across experiments in\nwarehouse and video-game environments. CP-Solver pro-\nvides an approximate solution for one-shot and lifelong\nMAPF among uncontrollable agents, enabling path planning\nwith safety guarantees by leveraging uncertainty quantifica-\ntion with predictions and adapting Enhanced Conflict-Based\nSearch. Future work could focus on robustness to distribu-\ntion shifts or enhancing prediction accuracy.\n\n--- Page 8 ---\nReferences\nAlahi, A.; Goel, K.; Ramanathan, V .; Robicquet, A.; Fei-Fei,\nL.; and Savarese, S. 2016. Social lstm: Human trajectory\nprediction in crowded spaces. In Proc. of the IEEE Conf. on\ncomputer vision and pattern recognition , 961\u2013971.\nAmes, A. D.; Coogan, S.; Egerstedt, M.; Notomista, G.;\nSreenath, K.; and Tabuada, P. 2019. Control barrier func-\ntions: Theory and applications. In 2019 18th European con-\ntrol Conf. (ECC) , 3420\u20133431. IEEE.\nAndreychuk, A.; Yakovlev, K.; Surynek, P.; Atzmon, D.; and\nStern, R. 2022. Multi-agent pathfinding with continuous\ntime. A.I., 305: 103662.\nAngelopoulos, A. N.; Barber, R. F.; and Bates, S. 2024.\nTheoretical Foundations of Conformal Prediction. arXiv\npreprint arXiv:2411.11824 .\nAngelopoulos, A. N.; and Bates, S. 2021. A gentle intro-\nduction to conformal prediction and distribution-free uncer-\ntainty quantification. arXiv preprint arXiv:2107.07511 .\nAtzmon, D.; Stern, R.; Felner, A.; Sturtevant, N. R.; and\nKoenig, S. 2020. Probabilistic robust MAPF. In Proc. of\nthe Int. Conf. on Automated Planning and Scheduling , vol-\nume 30, 29\u201337.\nAtzmon, D.; Stern, R.; Felner, A.; Wagner, G.; Bart \u00b4ak, R.;\nand Zhou, N.-F. 2018. Robust MAPF. In Proc. of the Int.\nSym. on Combinatorial Search , volume 9, 2\u20139.\nBansal, S.; Chen, M.; Herbert, S.; and Tomlin, C. J. 2017.\nHamilton-jacobi reachability: A brief overview and recent\nadvances. In 2017 IEEE 56th Annual Conf. on Decision and\nControl (CDC) , 2242\u20132253. IEEE.\nBarer, M.; Sharon, G.; Stern, R.; and Felner, A. 2014. Sub-\noptimal variants of the conflict-based search algorithm for\nthe multi-agent pathfinding problem. In Proc. of the Int.\nSym. on combinatorial Search , volume 5, 19\u201327.\nBellusci, M.; Basilico, N.; Amigoni, F.; et al. 2020. MAPF in\nconfigurable environments. In Proc. of the Intl. Joiint Conf.\non Autonomous Agents and Multiagent Systems , 159\u2013167.\nInt. Foundation for Autonomous Agents and Multiagent Sys.\nBerkenkamp, F.; and Schoellig, A. P. 2015. Safe and robust\nlearning control with Gaussian processes. In 2015 European\nControl Conf. (ECC) , 2496\u20132501. IEEE.\nCauchois, M.; Gupta, S.; Ali, A.; and Duchi, J. C. 2023. Ro-\nbust validation: Confident predictions even when distribu-\ntions shift. Journal of the American Statistical Association ,\n(just-accepted): 1\u201322.\nChen, Z.; Harabor, D. D.; Li, J.; and Stuckey, P. J. 2021.\nSymmetry breaking for k-robust MAPF. In Proc. of the AAAI\nConf. on A.I. , volume 35, 12267\u201312274.\nCleaveland, M.; Lee, I.; Pappas, G. J.; and Lindemann, L.\n2024. Conformal prediction regions for time series using\nlinear complementarity programming. In Proc. of the AAAI\nConf. on A.I. , volume 38, 20984\u201320992.\nHart, P. E.; Nilsson, N. J.; and Raphael, B. 1968. A formal\nbasis for the heuristic determination of minimum cost paths.\nIEEE Tran. on Sys. Science and Cybernetics , 4(2): 100\u2013107.\nHochreiter, S. 1997. Long Short-term Memory. Neural\nComputation MIT-Press .H\u00a8onig, W.; Kumar, T.; Cohen, L.; Ma, H.; Xu, H.; Aya-\nnian, N.; and Koenig, S. 2016. MAPF with kinematic con-\nstraints. In Proc. of the Int. Conf. on Automated Planning\nand Scheduling , volume 26, 477\u2013485.\nLaValle, S. M. 2006. Planning algorithms . Cambridge U.\nPress.\nLi, J.; Tinka, A.; Kiesel, S.; Durham, J. W.; Kumar, T. S.; and\nKoenig, S. 2021. Lifelong MAPF in large-scale warehouses.\nInProc. of the AAAI Conf. on A.I. , volume 35, 11272\u201311281.\nLindemann, L.; Cleaveland, M.; Shim, G.; and Pappas, G. J.\n2023. Safe planning in dynamic environments using confor-\nmal prediction. IEEE Robo. and Auto. Letters .\nLindemann, L.; Zhao, Y .; Yu, X.; Pappas, G. J.; and Desh-\nmukh, J. V . 2024. Formal verification and control with con-\nformal prediction. arXiv preprint arXiv:2409.00536 .\nMa, H.; Li, J.; Kumar, T. S.; and Koenig, S. 2017. Life-\nlong MAPF for Online Pickup and Delivery Tasks. In Proc.\nof the 16th Conf. on Autonomous Agents and MultiAgent\nSystems , AAMAS \u201917, 837\u2013845. Int. Foundation for Au-\ntonomous Agents and Multiagent Sys.\nNayakanti, N.; Al-Rfou, R.; Zhou, A.; Goel, K.; Refaat,\nK. S.; and Sapp, B. 2022. Wayformer: Motion forecasting\nvia simple & efficient attention networks. arXiv preprint\narXiv:2207.05844 .\nOkumura, K.; and Tixeuil, S. 2023. Fault-tolerant offline\nmulti-agent path planning. In Proc. of the AAAI Conf. on\nA.I., volume 37, 11647\u201311654.\nPadalkar, A.; Pooley, A.; Jain, A.; Bewley, A.; Herzog, A.;\nIrpan, A.; Khazatsky, A.; Rai, A.; Singh, A.; Brohan, A.;\net al. 2023. Open x-embodiment: Robotic learning datasets\nand rt-x models. arXiv preprint arXiv:2310.08864 .\nPhan, T.; Huang, T.; Dilkina, B.; and Koenig, S. 2024. Adap-\ntive Anytime MAPF Using Bandit-Based Large Neighbor-\nhood Search. In Proc. of the AAAI Conf. on A.I. , volume 38,\n17514\u201317522.\nRawlings, J. B. 2000. Tutorial overview of model predictive\ncontrol. IEEE control Sys. magazine , 20(3): 38\u201352.\nRober, N.; Katz, S. M.; Sidrane, C.; Yel, E.; Everett, M.;\nKochenderfer, M. J.; and How, J. P. 2023. Backward reacha-\nbility analysis of neural feedback loops: Techniques for lin-\near and nonlinear systems. IEEE Open Journal of Control\nSys.\nSharon, G.; Stern, R.; Felner, A.; and Sturtevant, N. R. 2015.\nConflict-based search for optimal multi-agent pathfinding.\nA.I., 219: 40\u201366.\nStankeviciute, K.; M Alaa, A.; and van der Schaar, M. 2021.\nConformal time-series forecasting. Adv. in Neural Info. Pro-\ncess. Sys. , 34: 6216\u20136228.\nStern, R.; Sturtevant, N. R.; Felner, A.; Koenig, S.; Ma, H.;\nWalker, T. T.; Li, J.; Atzmon, D.; Cohen, L.; Kumar, T.\nK. S.; Boyarski, E.; and Bartak, R. 2019. MAPF: Defini-\ntions, Variants, and Benchmarks. Sym. on Combinatorial\nSearch (SoCS) , 151\u2013158.\nStrawn, K.; and Ayanian, N. 2022. Byzantine fault toler-\nant consensus for lifelong and online multi-robot pickup and\n\n--- Page 9 ---\ndelivery. In Distributed Autonomous Robotic Systems: 15th\nInt. Sym. , 31\u201344. Springer.\nStrawn, K. J.; Ayanian, N.; and Lindemann, L. 2023. Con-\nformal Predictive Safety Filter for RL Controllers in Dy-\nnamic Environments. IEEE Robo. and Auto. Letters , 8(11):\n7833\u20137840.\nThrun, S. 2002. Probabilistic robotics. Communications of\nthe ACM , 45(3): 52\u201357.\nWan, Q.; Gu, C.; Sun, S.; Chen, M.; Huang, H.; and Jia,\nX. 2018. Lifelong MAPF in a dynamic environment. In\n2018 15th Int. Conf. on Control, Auto., Robo. and Vision\n(ICARCV) , 875\u2013882. IEEE.\nYu, J.; and LaValle, S. M. 2016. Optimal multirobot\npath planning on graphs: Complete algorithms and effective\nheuristics. IEEE Tran. on Robo. , 32(5): 1163\u20131177.",
  "project_dir": "artifacts/projects/CP_Solver_Multi_Agent_Path_Finding",
  "communication_dir": "artifacts/projects/CP_Solver_Multi_Agent_Path_Finding/.agent_comm",
  "assigned_at": "2025-07-31T21:00:56.948168",
  "status": "assigned"
}